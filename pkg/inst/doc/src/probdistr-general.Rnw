%----------------------------------------------------------------------------------------------------------
% 	Copyright (c) 2009 R-forge 'distributions' Core Team, 
% 	
%	The following Sweave code is under the GNU Free Documentation License:
%      	Permission is granted to copy, distribute and/or modify this document
%      	under the terms of the GNU Free Documentation License, Version 1.3
%      	or any later version published by the Free Software Foundation;
%      	with no Invariant Sections, no Front-Cover Texts, and no Back-Cover Texts.
%
%      A copy of the license is included in the 'inst' directory of this package 
%      or on the web at http://www.gnu.org/licenses/licenses.html#FDL
%
%	After running Sweave, the following code could be compiled :
%	  - on windows with a Tex distribution such as miktex (http://miktex.org) 
%		and a front end Latex editor such as texniccenter (http://www.toolscenter.org)
%	  - on mac os with a Tex distribution such as TexLive and a front end Latex
%	  	editor such as Texshop (http://www.uoregon.edu/~koch/texshop/)
%	  - on linux with a Tex distribution such as teTex (http://www.tug.org/teTeX/)
%	  	and a front end Latex editor such as emacs (http://www.gnu.org/software/emacs/)
%
%----------------------------------------------------------------------------------------------------------

\chapter{Generalization of common distributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Generalized hyperbolic distribution}
This part entirely comes from \cite{ghyp}.
\subsection{Characterization}
The first way to characterize generalized hyperbolic distributions is to say that the random vector $X$ follows a multivariate $\mathcal G\mathcal H$ distribution if
\begin{equation}\label{eq:ghd}
  X \stackrel{\mathcal{L}}{=} \mu + W \gamma + \sqrt{W} A Z
\end{equation}
where
\begin{enumerate}
\item $Z \sim \mcal N_k(\mathbf{0},I_k)$
\item $A \in \mathbb R^{d \times k}$
\item $ \mu, \gamma \in \mathbb R^{d}$
\item $W \geq 0$ is a scalar-valued random variable which is
  independent of $Z$ and has a Generalized Inverse Gaussian
  distribution, written $GIG(\lambda, \chi, \psi)$.
\end{enumerate}
Note that there are at least five alternative definitions leading to
different parametrizations. 

Nevertheless, the parameters of a $\mathcal G\mathcal H$ distribution given by the above
definition admit the following interpretation:
\begin{itemize}
\item $\lambda, \chi, \psi$ determine the shape of the distribution,
  that is, how much weight is assigned to the tails and to the
  center. In general, the larger those parameters the closer is the
  distribution to the normal distribution.
\item $\mu$ is the location parameter.
\item $\Sigma = A A'$ is the dispersion-matrix.
\item $\gamma$ is the skewness parameter. If $\gamma = 0$, then the
  distribution is symmetric around $\mu$.
\end{itemize}
Observe that the conditional distribution of $X | W = w$ is normal,
\begin{equation}\label{eq:mixture}
  X | W = w \sim\mcal N_d(\mu + w \, \gamma, w \Sigma),
\end{equation}

Another way to define a generalized hyperbolic distribution is to use the density.
Since the conditional distribution of $X$ given $W$ is Gaussian with
mean $\mu + W \gamma$ and variance $ W \Sigma$ the $\mathcal G\mathcal H$ density can be
found by mixing $X | W$ with respect to $W$.
\begin{eqnarray}\label{eq:fghyp}
  f_X(x) &=& \int_0^\infty f_{X|W}(x|w) \, f_W(w) \, dw \\ \nonumber
  &=& \int_0^\infty
  \frac{e^{(x-\mu)' \Sigma^{-1} \gamma}}
  {(2\pi)^{\frac{d}{2}} \left|\Sigma\right|^{\frac{1}{2}}w^{\frac{d}{2}}}
  \exp\left\{ -\frac{Q(x)}{2w}-\frac{\gamma \Sigma \gamma}{2/w} \right\}
  f_W(w)dw \\ \nonumber
  &=& \frac{(\sqrt{\psi/\chi})^\lambda
    (\psi + \gamma \Sigma \gamma)^{\frac{d}{2}-\lambda}}
  {(2\pi)^{\frac{d}{2}} \left|\Sigma\right| ^\frac{1}{2}
    K_\lambda(\sqrt{\chi\psi})}\times
  \frac{K_{\lambda - \frac{d}{2}}(
    \sqrt{(\chi + Q(x))(\psi + \gamma \Sigma \gamma)})\;
    e^{(x - \mu)'\Sigma^{-1} \gamma}}
  {(\sqrt{(\chi + Q(x))(\psi + \gamma \Sigma \gamma)})^{\frac{d}{2} - \lambda}},
\end{eqnarray}
where $K_\lambda(\cdot)$ denotes the modified Bessel
function of the third kind and $Q(x)$ denotes the mahalanobis distance $Q(x) =
(x - \mu)' \Sigma^{-1} (x - \mu)$ (i.e. the distance with $\Sigma^{-1}$ as norm).  
The domain of variation of the
parameters $\lambda, \chi$ and $\psi$ is given in section
\ref{sec:param}.

A last way to characterize generalized hyperbolic distributions is the usage of moment generating functions. An appealing property of normal mixtures is that the moment generating
function is easily calculated once the moment generating function of
the mixture is known. Based on equation (\ref{eq:moment-gen-gig}) we
obtain the moment generating function of a $\mathcal G\mathcal H$ distributed random
variable $X$ as
\begin{eqnarray}
\label{eq:moment-gen-gig}
    M(t) &=& E(E(\exp \left\{t' X\right\} | W)) =
    e^{t'\mu} E(\exp \left\{W \left(t' \gamma + 1/2~t'
          \Sigma t\right) \right\}  ) \nonumber \\
    &=& e^{t' \mu} \left(\frac{\psi}{\psi - 2 t'
        \gamma - t' \Sigma t}\right)^{\lambda / 2}
    \frac{K_\lambda(\sqrt{\psi (\chi - 2 t'
        \gamma - t' \Sigma t)})}{K_\lambda(\sqrt{\chi
        \psi})}, \quad \chi \geq 2~t' \gamma + t' \Sigma
    t. \nonumber
\end{eqnarray}
For moment generating functions of the special cases of the $\mathcal G\mathcal H$
distribution we refer to \cite{Prause1999} and \cite{Paolella2007}.

\subsection{Parametrization}\label{sec:param}
% <---------------------------------------------------------------------->
There are several alternative parametrizations for the GH
distribution.  In the~\soft{R}~package~\soft{ghyp}~the user can choose between
three of them.  There exist further parametrizations which are not
implemented and not mentioned here. For these parametrizations we
refer to \cite{Prause1999} and \cite{Paolella2007}.

Table \ref{tab:param} describes the parameter ranges for each
parametrization and each special case. Clearly, the dispersion
matrices $\Sigma$ and $\Delta$ have to fulfill the usual conditions
for covariance matrices, i.e., symmetry and positive definiteness as
well as full rank.

\begin{table}[h]
  \begin{small}
    \begin{tabular}{l | c c c c c c}
      & \multicolumn{6}{c}{$(\lambda,\chi,\psi,\mu,\Sigma,\gamma)$-Parametrization} \\
      &  $\lambda$ & $\chi$ & $\psi$ & $\mu$ & $\Sigma$ & $\gamma$\\
      \hline
      ghyp & $\lambda \in\mbb R$ & $\chi > 0$ & $\psi > 0$ & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      hyp & $\lambda = \frac{d + 1}{2} $ & $\chi > 0$ & $\psi > 0$ & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      NIG & $\lambda = - \frac{1}{2}$ & $\chi > 0$ & $\psi > 0$ & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      t & $\lambda < 0$ & $\chi > 0$ & $\psi = 0$ & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      VG & $\lambda > 0$ & $\chi = 0$ & $\psi > 0$ & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      \hline
      \\
      % -------------------------------------------------------------------------
      & \multicolumn{6}{c}{$(\lambda,\bar\alpha,\mu,\Sigma,\gamma)$-Parametrization} \\
      &  $\lambda$ & $\bar\alpha$ & $\mu$ & $\Sigma$ & $\gamma$\\
      \hline
      ghyp & $\lambda \in\mbb R$ & $\bar\alpha > 0$ & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      hyp & $\lambda = \frac{d + 1}{2} $ & $\bar\alpha > 0$  & $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      NIG & $\lambda = \frac{1}{2}$ & $\bar\alpha > 0$ &  $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      t & $\lambda = - \frac{\nu}{2} < -1$ & $\bar\alpha = 0$ &   $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      VG & $\lambda > 0$ & $\bar\alpha = 0$ &  $\mu \in\mbb R^d$ & $\Sigma \in\mbb R^\Sigma$ & $\gamma \in\mbb R^d$\\
      \hline
      \\
      % -------------------------------------------------------------------------
      & \multicolumn{6}{c}{$(\lambda,\alpha,\mu,\Sigma,\delta,\beta)$-Parametrization} \\
      &  $\lambda$ & $\alpha$ & $\delta$ & $\mu$ & $\Delta$ & $\beta$\\
      \hline
      ghyp & $\lambda \in\mbb R$ & $\alpha > 0$ & $\delta > 0$ & $\mu \in\mbb R^d$ & $\Delta \in\mbb R^\Delta$ & $\beta \in \{x \in\mbb R^d : \alpha^2 - x' \Delta x > 0\}$ \\
      hyp & $\lambda = \frac{d + 1}{2} $ & $\alpha > 0$ & $\delta > 0$ & $\mu \in\mbb R^d$ & $\Delta \in\mbb R^\Delta$  & $\beta \in \{x \in\mbb R^d : \alpha^2 - x' \Delta x > 0\}$ \\
      NIG & $\lambda = - \frac{1}{2}$ & $\alpha > 0$ & $\delta > 0$ & $\mu \in\mbb R^d$ & $\Delta \in\mbb R^\Delta$  & $\beta \in \{x \in\mbb R^d : \alpha^2 - x' \Delta x > 0\}$\\
      t & $\lambda < 0$ & $\alpha = \sqrt{\beta' \Delta \beta}$ & $\delta > 0$ & $\mu \in\mbb R^d$ & $\Delta \in\mbb R^\Delta$  & $\beta \in\mbb R^d$ \\
      VG & $\lambda > 0$ & $\alpha > 0$ & $\delta = 0$ & $\mu \in\mbb R^d$ & $\Delta \in\mbb R^\Delta$  & $\beta \in \{x \in\mbb R^d : \alpha^2 - x' \Delta x > 0\}$\\
      \hline
    \end{tabular}
  \end{small}
  \caption{The domain of variation for the parameters of the GH distribution and
    some of its special cases for different parametrizations. We denote the set of all feasible covariance
    matrices in $\mbb R^{d \times d}$ with $\mbb R^\Sigma$.
    Furthermore, let $\mbb R^\Delta = \{A \in\mbb R^\Sigma : |A| = 1\}$.}\label{tab:param}
\end{table}

Internally, he package \soft{ghyp}~uses the
$(\lambda,\chi,\psi,\mu,\Sigma,\gamma)$-parametrization. However, fitting is done in the
($\lambda,\bar\alpha,\mu,\Sigma,\gamma$)-parametrization since this parametrization does not
necessitate additional constraints to eliminate the redundant degree
of freedom.  Consequently, what cannot be represented by the
($\lambda,\alpha,\mu,\Sigma,\delta,\beta$)-parametrization cannot be fitted (cf. section
\ref{sec:lambdaabarparam}).
% <---------------------------------------------------------------------->
\subsubsection{$(\lambda,\chi,\psi,\mu,\Sigma,\gamma)$-Parametrization}\label{sec:chi-psi-param}
% <---------------------------------------------------------------------->
The ($\lambda,\chi,\psi,\mu,\Sigma,\gamma$)-parametrization is obtained as the normal
mean-variance mixture distribution when $W \sim GIG(\lambda, \chi,
\psi)$.  This parametrization has a drawback of an identification
problem. Indeed, the distributions $GH_d(\lambda,\chi,\psi,\mu,\Sigma,\gamma)$ and 
$GH_d(\lambda,\chi/k,k\psi,\mu,k\Sigma,k\gamma)$ are identical for
any $k > 0$. Therefore, an identifying problem occurs when we start to
fit the parameters of a GH distribution to data. This problem could be
solved by introducing a suitable contraint. One possibility is to
require the determinant of the dispersion matrix $\Sigma$ to be $1$.
% <---------------------------------------------------------------------->
\subsubsection{($\lambda,\bar\alpha,\mu,\Sigma,\gamma$)-Parametrization}\label{sec:lambdaabarparam}
% <---------------------------------------------------------------------->
There is a more elegant way to eliminate the degree of freedom. We
simply constrain the expected value of the generalized inverse
Gaussian distributed mixing variable $W$ to be $1$
(cf. \ref{sec:gig}). This makes the interpretation of the skewness
parameters $\gamma$
easier and in addition, the fitting procedure becomes faster (cf. \ref{sec:EM}). \\
We define
\begin{equation}
  E(W)  =  \sqrt{\frac{\chi}{\psi}}\frac{K_{\lambda+1}(\sqrt{\chi\psi})}
  {K_{\lambda}{(\sqrt{\chi\psi})}} = 1.
\end{equation}
and set
\begin{equation}
  \bar\alpha = \sqrt{\chi\psi}.
\end{equation}
It follows that
\begin{equation} \label{eq:abartochipsi} \psi = \bar\alpha \;
  \frac{K_{\lambda+1}(\bar\alpha)}{K_{\lambda} (\bar\alpha)} \;
  \hbox{and} \; \chi = \frac{\bar\alpha^2}{\psi}= \bar\alpha \;
  \frac{K_{\lambda}(\bar\alpha)}{K_{\lambda+1} (\bar\alpha)}.
\end{equation}
The drawback of the ($\lambda,\bar\alpha,\mu,\Sigma,\gamma$)-parametrization is that it does
not exist in the case $\bar\alpha = 0$ and $\lambda \in [-1,0]$, which
corresponds to a Student-t distribution with non-existing
variance. Note that the ($\lambda,\bar\alpha,\mu,\Sigma,\gamma$)-parametrization yields to a
slightly different parametrization for the special case of a Student-t
distribution.
% <---------------------------------------------------------------------->
\subsubsection{$(\lambda,\alpha,\mu,\Sigma,\delta,\beta)$-Parametrization}
% <---------------------------------------------------------------------->
When the GH distribution was introduced in \citet{Barndorffnielsen77},
the following parametrization for the multivariate case was used.
\begin{equation}
  f_X(x)  = \frac{ (\alpha^2-\beta'\Delta\beta)^{\lambda /2}}{(2\pi)^{\frac{d}{2}} \sqrt{|\Delta|} \, \delta^\lambda \,
    K_{\lambda}(\delta \sqrt{\alpha^2-\beta'\Delta\beta})}\times
  \frac{K_{\lambda - \frac{d}{2}}(\alpha \sqrt{\delta^2 + (x-\mu)'\Delta^{-1}(x-\mu)}) \;
    e^{\beta' (x - \mu)}}{(\alpha \sqrt{\delta^2 + (x-\mu)'\Delta^{-1}(x-\mu)})^{\frac{d}{2} - \lambda}},
\end{equation}
where the determinant of $\Delta$ is constrained to be $1$.  In the
univariate case the above expression reduces to
\begin{equation}
  f_X(x)  = \frac{ (\alpha^2 - \beta^2)^{\lambda /2}}
  {\sqrt{2\pi} \, \alpha^{\lambda - \frac{1}{2}} \, \delta^\lambda \,
    K_{\lambda}(\delta \sqrt{\alpha^2 - \beta^2})}\times
  K_{\lambda - \frac{1}{2}}(\alpha \sqrt{\delta^2 + (x - \mu)^2}) \;
  e^{\beta (x - \mu)},
\end{equation}
which is the most widely used parametrization of the GH distribution
in literature.
% <---------------------------------------------------------------------->
\subsubsection{Switching between different parametrizations}
% <---------------------------------------------------------------------->
The following formulas can be used to switch between the
($\lambda,\bar\alpha,\mu,\Sigma,\gamma$), ($\lambda,\chi,\psi,\mu,\Sigma,\gamma$), and the
($\lambda,\alpha,\mu,\Sigma,\delta,\beta$)-parametrization. The parameters $\lambda$
and $\mu$ remain the same, regardless of the parametrization. \\
The way to obtain the ($\lambda,\alpha,\mu,\Sigma,\delta,\beta$)-parametrization from the
($\lambda,\bar\alpha,\mu,\Sigma,\gamma$)-parametrization yields over the
($\lambda,\chi,\psi,\mu,\Sigma,\gamma$)-parametrization:
$$(\lambda,\bar\alpha,\mu,\Sigma,\gamma) \quad \leftrightarrows \quad (\lambda,\chi,\psi,\mu,\Sigma,\gamma)
\quad \leftrightarrows \quad (\lambda,\alpha,\mu,\Sigma,\delta,\beta)$$
\begin{description}
\item[$(\lambda,\bar\alpha,\mu,\Sigma,\gamma) \rightarrow (\lambda,\chi,\psi,\mu,\Sigma,\gamma)$:] Use the relations
  in (\ref{eq:abartochipsi}) to obtain $\chi$ and $\psi$.  The
  parameters $\Sigma$ and $\gamma$ remain the same.
\item[$(\lambda,\chi,\psi,\mu,\Sigma,\gamma) \rightarrow (\lambda,\bar\alpha,\mu,\Sigma,\gamma)$:] Set $k =
  \sqrt{\frac{\chi}{\psi}}\frac{K_{\lambda+1}(\sqrt{\chi\psi})}
  {K_{\lambda}{(\sqrt{\chi\psi})}}.$
  \begin{equation}
    \bar\alpha = \sqrt{\chi\psi} , \quad \Sigma \equiv k \, \Sigma, \quad \gamma \equiv k \, \gamma
  \end{equation}
\item[$(\lambda,\chi,\psi,\mu,\Sigma,\gamma) \rightarrow (\lambda,\alpha,\mu,\Sigma,\delta,\beta)$:]
  \begin{eqnarray}
    \Delta = |\Sigma|^{-\frac{1}{d}} \Sigma &,& \beta = \Sigma^{-1} \gamma \nonumber \\
    \delta = \sqrt{\chi |\Sigma|^{\frac{1}{d}}} &,& \alpha =
    \sqrt{|\Sigma|^{-\frac{1}{d}} (\psi + \gamma'\Sigma^{-1}\gamma)}
  \end{eqnarray}
\item[$(\lambda,\alpha,\mu,\Sigma,\delta,\beta) \rightarrow (\lambda,\chi,\psi,\mu,\Sigma,\gamma)$:]
  \begin{equation}
    \Sigma = \Delta , \quad \gamma = \Delta \beta , \quad \chi =
    \delta^2 ,
    \quad \psi = \alpha^2 - \beta' \Delta \beta .
  \end{equation}
\end{description}
%---------------------------%---------------------------%---------------------------%---------------------------

\subsection{Properties}
\subsubsection{Moments}
The expected value and the variance are given by
\begin{eqnarray}
\label{eq:momentgig}
  E(X)  &=& \mu + E(W) \gamma \\
  Var(X)  &=& E(Cov(X | W)) + Cov(E(X| X)) \\ \nonumber
  &=& Var(W) \gamma \gamma'  + E(W) \Sigma.
\end{eqnarray}

\subsubsection{Linear transformation}
The GH class is closed under linear transformations:
%\begin{proposition}
  If $X \sim GH_d(\lambda,\chi,\psi,\mu,\Sigma,\gamma)$ and $Y = B X + \mathbf{b}$, where $B \in
  \mbb R^{k \times d}$ and $\mathbf{b} \in \mbb R^k$, then $Y
  \sim GH_k(\lambda,\chi,\psi,B\mu+\mathbf{b},B\Sigma B',B\gamma)$.
%\end{proposition}
%\begin{proof}
%  The characteristic function of $X$ is
%  $$\phi_X(\tvec) = E\left(E\left(e^{i \, \tvec' X\right) | W\right) =
%  E\left(e^{i \, \tvec' (\mu + W \gamma) - 1/2 \, W \, \tvec' \, \Sigma
%      \, \tvec}\right) = e^{i \, \tvec' \mu} \, \hat{H}(1/2 \, \tvec' \,
%  \Sigma \, \tvec - i \, \tvec' \gamma),$$ where $\hat{H}(\theta) =
%  \int_0^\infty e^{-\theta v} dH(v)$ denotes the Laplace-Stieltjes
%  transform of the distribution function $H$ of $W$. Let $\Y = B \, X
%  + \mathbf{b}$. The characteristic function is then
%  \begin{eqnarray*}
%    \phi_Y(t) &=& E\left(E\left(e^{i \, \tvec' \, (B \, X  + \mathbf{b})} | W\right)\right) =
%    E\left(e^{i \, \tvec' \, (B (\mu + W \gamma) + \mathbf{b}) -
%        1/2 \, W \,\tvec' \, B \, \Sigma \, B' \, \tvec}\right)\\
%    &=& e^{i \, \tvec' \, (B \, \mu + \mathbf{b})} \, \hat{H}(1/2 \, W
%    \, \tvec' \, B \, \Sigma \, B' \, \tvec -  i \, \tvec' B \, \gamma).
%  \end{eqnarray*}
%  Therefore, $B X + \mathbf{b} \sim \GHtransf$.
%\end{proof}
Observe that by introducing a new skewness parameter $\bar{\gamma} =
\Sigma \gamma$, all the shape and skewness parameters ($\lambda, \chi,
\psi, \bar{\gamma}$) become location and scale-invariant, provided the
transformation does not affect the dimensionality, that is
$B \in\mbb R^{d \times d}$ and $\mathbf{b} \in\mbb R^d$.


\subsection{Special cases}\label{sec:specialcases}
% <---------------------------------------------------------------------->
The GH distribution contains several special cases known under special
names.
\begin{itemize}
\item{If $\lambda = \frac{d+1}{2}$ the name generalized is dropped and
    we have a multivariate \emph{hyperbolic (hyp)} distribution. The
    univariate margins are still GH distributed. Inversely, when
    $\lambda = 1$ we get a multivariate GH distribution with
    hyperbolic margins.}
\item{If $\lambda = -\frac{1}{2}$ the distribution is called
    \emph{Normal Inverse Gaussian (NIG)}.}
\item{If $\chi = 0$ and $\lambda > 0$ one gets a limiting case which
    is known amongst others as \emph{Variance Gamma (VG)}
    distribution.}
\item{If $\psi = 0$ and $\lambda < -1$ one gets a limiting case which
    is known as a \emph{generalized hyperbolic Student-t} distribution
    (called simply \emph{Student-t} in what follows).}
\end{itemize}
%Further information about the special cases and the necessary formulas
%to fit these distributions to multivariate data can be found in the
%appendixes \ref{sec:gig} and \ref{sec:dens_special_cases}. The
%parameter constraints for the special cases in different
%parametrizations are described in the following section.

\subsection{Estimation}
% <---------------------------------------------------------------------->
%\section{Fitting generalized hyperbolic distributions to data}
% <---------------------------------------------------------------------->
Numerical optimizers can be used to fit univariate GH
distributions to data by means of maximum likelihood estimation.
Multivariate GH distributions
can be fitted with expectation-maximazion (EM) type algorithms (see
\citet{Dempster.1977} and \citet{Meng.1993}).
%-------------------------------------------------------------------------
\subsubsection{EM-Scheme}\label{sec:EM}
% <---------------------------------------------------------------------->
Assume we have iid data $x_1,\ldots,x_n$ and parameters represented
by $\Theta = (\lambda,\bar\alpha,\mu,\Sigma,\gamma)$. The problem is to maximize
\begin{equation}
  \ln L(\Theta;x_1,\ldots,x_n) = \sum_{i=1}^n \ln f_X(x_i;\Theta).
\end{equation}
This problem is not easy to solve due to the number of parameters and
necessity of maximizing over covariance matrices.  We can proceed by
introducing an augmented likelihood function
\begin{equation}\label{eq:likelihood}
  \ln \tilde{L}(\Theta;x_1,\ldots,x_n,w_1,\ldots,w_n) =
  \sum_{i=1}^n \ln f_{X|W}(x_i|w_i;\mu,\Sigma,\gamma) +
  \sum_{i=1}^n \ln f_W(w_i;\lambda,\bar\alpha)
\end{equation}
and spend the effort on the estimation of the latent mixing variables
$w_i$ coming from the mixture representation (\ref{eq:mixture}).  This
is where the EM algorithm comes into play.
\begin{description}
\item[\textbf{E-step:}]{Calculate the conditional expectation of the
    likelihood function (\ref{eq:likelihood}) given the data
    $x_1,\ldots,x_n$ and the current estimates of parameters
    $\Theta^{[k]}$}.  This results in the objective function
  \begin{equation}
    Q(\Theta;\Theta^{[k]})=E\left(
      \ln \tilde{L}(\Theta;x_1,\ldots,x_n,w_1,\ldots,w_n)|
      x_1,\ldots,x_n;\Theta^{[k]}\right).
  \end{equation}

\item[\textbf{M-step:}]{Maximize the objective function with respect
    to $\Theta$ to obtain the next set of estimates $\Theta^{[k+1]}$.}
\end{description}
Alternating between these steps yields to the maximum likelihood
estimation of the parameter set $\Theta$. In practice,
performing the E-Step means maximizing the second summand of
(\ref{eq:likelihood}) numerically. The log density of the GIG
distribution (cf. \ref{eq:densgig}) is
\begin{equation}\label{eq:logfgig}
  \ln f_W(w) = \frac{\lambda}{2} \ln(\psi/\chi) - \ln(2 K_\lambda(\sqrt{\chi\psi}) )
  + (\lambda - 1) \ln w -\frac{\chi}{2} \frac{1}{w} - \frac{\psi}{2} w.
\end{equation}
When using the ($\lambda,\bar\alpha$)-parametrization this problem is of
dimension two instead of three as it is in the ($\lambda, \chi,
\psi$)-parametrization.
As a consequence the performance increases.\\
Since the $w_i$'s are latent one has to replace $w$, $1/w$ and $\ln w$
with the respective expected values in order to maximize the log
likelihood function.  Let
\begin{equation} \label{eq:etadeltaxi} \eta_i^{[k]} := E\left(w_i \, | \,
    x_i;\Theta^{[k]}\right),\; \delta_i^{[k]} := E\left(w_i^{-1} \, | \,
    x_i;\Theta^{[k]}\right),\; xi_i^{[k]} := E\left(\ln w_i \, | \, x_i;\Theta^{[k]}\right).
\end{equation}
We have to find the conditional density of $w_i$ given $x_i$ to
calculate these quantities.
% <---------------------------------------------------------------------->
\subsubsection{MCECM estimation}
% <---------------------------------------------------------------------->
In the \texttt{R} implementation a modified EM scheme is used, which
is called multi-cycle, expectation, conditional estimation (MCECM)
algorithm \citep*{Meng.1993, QRMmcneil05}.  The different steps of the
MCECM algorithm are sketched as follows:
\renewcommand{\labelenumi}{(\arabic{enumi})}
\begin{enumerate}
\item{Select reasonable starting values for $\Theta^{[k]}$.  For example
    $\lambda = 1$, $\bar\alpha= 1$, $\mu$ is set to the sample mean,
    $\Sigma$ to the sample covariance matrix and $\gamma$ to a zero
    skewness vector.  }
\item{Calculate $\chi^{[k]}$ and $\psi^{[k]}$ as a function of
    $\bar\alpha^{[k]}$ using (\ref{eq:abartochipsi}).  }
\item{Use (\ref{eq:etadeltaxi}), (\ref{eq:momentgig}) to calculate the weights $\eta_i^{[k]}$ and
    $\delta_i^{[k]}$. Average the weights to get
    \begin{equation}
      \bar{\eta}^{[k]} = \frac{1}{n} \sum_{i=1}^{n} \eta_i^{[k]}
      \; \hbox{and} \;
      \bar{\delta}^{[k]} = \frac{1}{n} \sum_{i=1}^{n} \delta_i^{[k]}.
    \end{equation}
  }
\item{If an asymmetric model is to be fitted set $\gamma$ to
    $\mathbf{0}$, else set
    \begin{equation}
      \gamma^{[k+1]}= \frac{1}{n} \frac{\sum_{i=1}^{n} \delta_i^{[k]} (\bar{x}-x_i)}
      {\bar{\eta}^{[k]} \bar{\delta}^{[k]} - 1}.
    \end{equation}
  }
\item{Update $\mu$ and $\Sigma$:
    \begin{eqnarray}
      \mu^{[k+1]} &=& \frac{1}{n} \frac{\sum_{i=1}^{n} \delta_i^{[k]} (x_i-\gamma^{[k+1]})}
      {\bar{\delta}^{[k]}}\\
      \Sigma^{[k+1]} &=& \frac{1}{n} \sum_{i=1}^{n} \delta_i^{[k]} (x_i-\mu^{[k+1]})(x_i-\mu^{[k+1]})'
      - \bar{\eta}^{[k]} \gamma^{[k+1]} \gamma^{[k+1]}\,'.
    \end{eqnarray}
  }
\item{Set $\Theta^{[k,2]} = (\lambda^{[k]}, \bar\alpha^{[k]},\mu^{[k+1]},
    \Sigma^{[k+1]}, \gamma^{[k+1]})$ and calculate weights
    $\eta_i^{[k,2]}$, $\delta_i^{[k,2]}$ and $xi_i^{[k,2]}$ using
    (\ref{eq:etadeltaxi}), (\ref{eq:eloggig}) and
    (\ref{eq:momentgig}).  }
\item{Maximize the second summand of (\ref{eq:likelihood}) with
    density (\ref{eq:logfgig}) with respect to $\lambda$, $\chi$ and
    $\psi$ to complete the calculation of $\Theta^{[k,2]}$ and go back
    to step (2). Note that the objective function must calculate
    $\chi$ and $\psi$ in dependence of $\lambda$ and $\bar\alpha$ using
    relation (\ref{eq:abartochipsi}).  }
\end{enumerate}
% <---------------------------------------------------------------------->



\subsection{Random generation}
We can simply use the first characterization by adding
$$
 \mu + W \gamma + \sqrt{W} A Z
$$
where $Z$ is a multivariate gaussian vector $\mcal N_k(\mathbf{0},I_k)$ and $W$ follows a 
Generalized Inverse Gaussian $GIG(\lambda, \chi, \psi)$.


\subsection{Applications}
Even though the GH distribution was initially ivented to study the
distribution of the logarithm of particle sizes, we will focus on
applications of the GH distribution family in finance and risk
measurement.

We have seen above that the GH distribution is very flexible in the
sense that it nests several other distributions such as the Student-t
(cf. \ref{sec:studentt}).

To give some references and applications of the GH distribution let us
first summarize some of its important properties. Beside of the above
mentioned flexibility, three major facts led
to the popularity of GH distribution family in finance:
\begin{enumerate}
\item The GH distribution features both \emph{fat tails} and
  \emph{skewness}. These properties account for the
  stylized facts of financial returns.
\item The GH family is naturally extended to multivariate
  distributions\footnote{The extension to multivariate distributions
    is natural because of the mixing structure (see
    eq. (\ref{eq:mixture})).}. A multivariate GH distribution does
  exhibit some kind of \emph{non-linear dependence},
  for example \emph{tail-dependence}. This reflects the fact that extremes
  mostly occure for a couple of risk-drivers simultaneously in
  financial markets. This property is of fundamental importance for
  risk-management, and can influence for instance the asset allocation
  in portfolio theory.
\item The GH distribution is infinitely divisible
  (cf. \cite{Halgreen.1977}). This is a necessary and sufficient
  condition to build \emph{L\'evy processes}. L\'evy
  processes are widespread in finance because of their time-continuity and their ability to model jumps.
\end{enumerate}

Based on these properties one can classify the applications of the GH
distributions into the fields \emph{empirical modelling}, \emph{risk
  and dependence modelling}, \emph{derivative pricing}, and
\emph{portfolio selection}.

In the following, we try to assign papers to each of the classes of
applications mentioned above. Rather than giving abstracts for each
paper, we simply cite them and refer the interested reader to the
bibliography and to the articles. Note that some articles deal with
special cases of the GH distribution only.

\paragraph{Empirical modelling}
\citet{EberleinKeller95,BarndorffNielsen.2001,Fergusson.2006}
\paragraph{Risk and dependence modelling}
\citet{EberleinKellerPrause98,BreymannDiEm03,QRM2005,Chen.2005,Kassberger.2006}
\paragraph{L\'evy processes}
\citet{Barndorffnielsen97,Barndorffnielsen97b,Bibby.1997,Madan.1998,Raible2000,Cont.2003}
\paragraph{Portfolio selection}
\citet{Kassberger.2007}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Stable distribution}
A detailed and complete review of stable distributions can be found in \cite{nolan:2009}.
\subsection{Characterization}
Stable distributions are characterized by the following equation
$$
a\tilde X+b\underset{\tilde{}}{X} \stackrel{\mcal L}{=} cX+d,
$$
where $\tilde X$ and $\underset{\tilde{}}{X}$ are independent copies of a random variable $X$ and some positive constants $a, b, c$ and $d$. This equation means stable distributions are distributions closed for linear combinations. For the terminology, we say $X$ is strictly stable if $d=0$ and symmetric stable if in addition we have $X\stackrel{\mcal L}{=} -X$.
From \cite{nolan:2009}, we learn we use the word stable since the shape of the distribution is preserved under linear combinations.

Another way to define stable distribution is to use characteristic functions. $X$ has a stable distribution if and only if 
its characteristic function is
$$
\phi(t) = e^{it \delta}\times\left\{
\begin{array}{ll}
e^{-|t \gamma |^\alpha(1-i\beta\tan(\frac{\pi\alpha}{2})\sign(t))} & \txtm{if}\alpha\neq 1\\
e^{-|t \gamma |(1+i\beta\frac{2}{\pi}\log|t |\sign(t))} & \txtm{if}\alpha= 1\\
\end{array}
\right. ,
$$
where $\alpha\in]0,2]$, $\beta\in]-1,1[$, $\gamma>0$ and $b\in\mbb R$ are the parameters. In the following, we denote $\mcal S(\alpha,\beta, \gamma,\delta)$, where
$\delta $ is a location parameter, $\gamma$ a scale parameter, $\alpha$ an index of stability and $\beta$ a skewness parameter. This corresponds to the parametrization 1 of \cite{nolan:2009}.

We know that stable distributions $\mcal S(\alpha,\beta, \gamma,\delta)$ are continuous distributions whose support is 
$$
\left\{
\begin{array}{cl}
[\delta,+\infty[ & \txtm{if} \alpha<1 \txtm{and} \beta=1\\
]-\infty, \delta] & \txtm{if} \alpha<1 \txtm{and} \beta=-1\\
]-\infty, +\infty[ & \txtm{otherwise}
\end{array}
\right. .
$$

\subsection{Properties}
If we work with standard stable distributions $\mcal S(\alpha,\beta, 0,1)$, we have the reflection property. That is to say if $X\sim \mcal S(\alpha,\beta, 0,1)$, then $-X\sim \mcal S(\alpha,-\beta, 0,1)$. This implies the following constraint on the density and the distribution function:
$$
f_X(x) = f_{-X}(-x) \txtm{and}
F_X(x) = 1-F_{-X}(x).$$

From the definition, we have the obvious property on the sum. If $X$ follows a stable distribution $\mcal S(\alpha,\beta, \gamma,\delta)$, then $aX+b$ follows a stable distribution of parameters
$$
\left\{
\begin{array}{cl}
\mcal S(\alpha,\sign(a)\beta, |a|\gamma,a\delta+b) & \txtm{if} \alpha\neq 1\\
\mcal S(1,\sign(a)\beta, |a|\gamma,a\delta+b-\frac{2}{\pi}\beta\gamma a\log|a|) & \txtm{if} \alpha= 1
\end{array}
\right. .
$$ 

Furthermore if $X_1$ and $X_2$ follow a stable distribution $\mcal S(\alpha,\beta_i, \gamma_i,\delta_i)$ for $i=1,2$, then 
the sum $X_1+X_2$ follows a stable distribution $\mcal S(\alpha,\beta, \gamma,\delta)$ with $\beta=\frac{\beta_1\gamma_1^\alpha+\beta_2\gamma_2^\alpha}{\gamma_1^\alpha+\gamma_2^\alpha}$, $\gamma=(\gamma_1^\alpha+\gamma_2^\alpha)^{\frac{1}{\alpha}}$ and
$\delta=\delta_1+\delta_2$.

\subsection{Special cases}
The following distributions are special cases of stable distributions:
\begin{itemize}
\item $\mcal S(2,0,\sigma/\sqrt 2,\mu)$ is a Normal distribution defined by the density $f(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$,
\item $\mcal S(1,0,\gamma,\delta)$ is a Cauchy distribution defined by the density $f(x) = \frac{1}{\pi}\frac{\gamma}{\gamma^2+(x-\gamma)^2}$,
\item $\mcal S(1/2,1,\gamma,\delta)$ is a Lévy distribution defined by the density $f(x) = \sqrt{\frac{\gamma}{2\pi}} \frac{1}{(x-\delta)^{\frac{3}{2}}} e^{-\frac{\gamma}{2(x-\delta)}}$.
\end{itemize}

\subsection{Estimation}
NEED REFERENCE
\subsection{Random generation}
Simulation of stable distributions are carried out by the following algorithm from \cite{chambers}.
Let $\Theta$ be an independent random uniform variable $\mcal U(-\pi/2,\pi/2)$ and $W$ be an exponential variable with mean 1 independent from $\Theta$. For $0<\alpha\leq 2$, we have 
\begin{itemize}
\item in the symmetric case, 
$$
Z = \frac{\sin(\alpha\Theta)}{\cos(\Theta)^{\frac{1}{\alpha}}} \left(\frac{\cos((\alpha-1)\Theta)}{W} \right)^{\frac{1-\alpha}{\alpha}}
$$ follows a stable distribution $\mcal S(\alpha,0, 1,0)$ with the limiting case $\tan(\Theta)$ when $\alpha\rightarrow 1$.
\item in the nonsymetric case,
$$
Z = \left\{
\begin{array}{ll}
\frac{\sin(\alpha(\Theta+\theta))}{(\cos(\alpha\theta)\cos(\Theta))^{\frac{1}{\alpha}}} \left(\frac{\cos(\alpha\theta+(\alpha-1)\Theta)}{W} \right)^{\frac{1-\alpha}{\alpha}}\\
\frac{2}{\pi} \left((\frac{\pi}{2}+\beta\Theta)\tan(\Theta)-\beta\log\left(\frac{\frac{\pi}{2}W\cos(\Theta)}{\frac{\pi}{2}+\beta\Theta}\right)\right)
\end{array}
\right.
$$
follows a stable distribution $\mcal S(\alpha,\beta, 1,0)$ where $\theta=\arctan(\beta\tan(\pi\alpha/2))/\alpha$.
\end{itemize}
Then we get a ``full'' stable distribution with $\gamma Z+\delta$.


\subsection{Applications}
NEED REFERENCE

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Phase-type distribution}
\subsection{Characterization}
A phase-type distribution $PH(\pi,T,m)$ ($\pi$ a row vector of $\mathbb R^m$, $T$ a $m\times m$ matrix) 
is defined as the distribution of the time to absorption in the state $0$
of a Markov jump process, on the set $\{ 0,1,\dots,m\}$, with initial probability $(0,\pi)$ and intensity
matrix\footnote{matrix such that its row sums are equal to $0$ and have positive elements except on its diagonal.} 
$$
\Lambda= (\lambda_{ij})_{ij} = 
\left(
\begin{array}{c|c}
0 & 0\\
\hline
t_0 & T
\end{array}
\right),
$$
where the vector $t_0$ is $-T \mathbf{1}_m$ and $\mathbf{1}_m$ stands for the column vector of $1$ in $\mathbb R^m$.
This means that if we note $(M_t)_t$ the associated Markov process of a phase-type distribution, then we have
$$
P(M_{t+h}=j/ M_t=i) =
\left\{ 
\begin{array}{ll}
\lambda_{ij} h+o(h) & ~\textrm{if}~i\neq j\\
1+\lambda_{ii}h+o(h) & ~\textrm{if}~i=j\\
\end{array}
\right. .
$$
The matrix $T$ is called the sub-intensity matrix and $t_0$ the exit rate vector. 

The cumulative distribution function of a phase-type distribution is given by
$$
F(x) = 1-\pi e^{T x} \mathbf 1_m,
$$
and its density by
$$
f(x) = \pi e^{T x} t_0,
$$
where $e^{T x}$ denote the matrix exponential defined as the matrix serie $\sum\limits_{n=0}^{+\infty} \frac{T^nx^n}{n!}$. 

The computation of matrix exponential is studied in details in appendix \ref{computexpm}, but let us notice that when $T$ is a diagonal matrix, the matrix exponential is the exponential of its diagonal terms.
Let us note that there also exists discrete phase-type distribution, cf. \cite{bibbioetal}.


\subsection{Properties}
The moments of a phase-type distribution are given by $(-1)^n n! \pi T^{-n} \mathbf 1$. Since phase-type distributions
are platikurtic or light-tailed distributions, the Laplace transform exists 
$$
\widehat f(s) = \pi (-s I_m-T)^{-1} t_0,
$$
where $I_m$ stands for the $m\times m$ identity matrix. 

One property among many is the set of phase-type distributions is dense with the set of positive random variable distributions. Hence,
the distribution of any positive random variable can be written as a limit of phase-type distributions. However, a distribution can be
represented (exactly) as a phase-type distribution if and only if the three following conditions are verified
\begin{itemize}
\item the distribution has a rational Laplace transform;
\item the pole of the Laplace transform with maximal real part is unique;
\item it has a density which is positive on $\mathbb R_+^\star$.
\end{itemize}


\subsection{Special cases}
Here are some examples of distributions, which can be represented by a phase-type distribution
\begin{itemize}
\item exponential distribution $\mathcal E(\lambda)$ : $\pi=1$, $T=-\lambda$ and $m=1$.
\item generalized Erlang distribution $\mathcal G\left(n,(\lambda_i)_{1\leq i \leq n}\right)$ : $$\pi=(1,0,\dots,0),$$
$$
T=
\left(\begin{array}{ccccc}
-\lambda_1&\lambda_1&0&\dots&0\\
0&-\lambda_2&\lambda_2&\ddots&0\\
0&0&-\lambda_3&\ddots&0\\
0&0&\ddots&\ddots&\lambda_{n-1}\\
0&0&0&0&-\lambda_{n}\\
\end{array}\right),
$$
and $m=n$.
\item a mixture of exponential distribution of parameter $(p_i,\lambda_i)_{1\leq i \leq n}$ : $$\pi=(p_1,\dots,p_n),$$
$$
T=
\left(\begin{array}{ccccc}
-\lambda_1&0&0&\dots&0\\
0&-\lambda_2&0&\ddots&0\\
0&0&-\lambda_3&\ddots&0\\
0&0&\ddots&\ddots&0\\
0&0&0&0&-\lambda_n\\
\end{array}\right),
$$
and $m=n$.
\item a mixture of 2 (or $k$) Erlang distribution $\mathcal G(n_i,\lambda_i)_{i=1,2}$ with parameter $p_i$ : 
$$\pi = (\underbrace{p_1,0,\dots,0}_{n_1},\underbrace{p_2,0,\dots,0}_{n_2}),$$
$$
T=
\left(\begin{array}{ccccccc}
-\lambda_1&\lambda_1&0&0&\dots&0&0\\
0&\ddots&\lambda_1&0&\ddots&0&0\\
0&0&-\lambda_1&0&0&0&0\\
0&0&\ddots&-\lambda_2&\lambda_2&0&0\\
0&0&0&0&\ddots&\ddots&0\\
0&0&0&0&0&\ddots&\lambda_2\\
0&0&0&0&0&0&-\lambda_2\\
\end{array}\right),
$$
and $m=n_1+n_2$.
\end{itemize}

\subsection{Estimation}
The estimation based on moments can be a starting point for parameters, but according to \cite{feldmanwhit} the fit is very poor. \cite{feldmanwhit} proposes a recursive algorithm matching theoretical quantiles and empirical quantiles. They illustrates their method with the Weibull and the Pareto distribution by a mixture of exponential distributions.

First \cite{asmussenolhsson} and then \cite{leelin} fit phase-type distribution with the EM algorithm. \cite{leelin} also investigates goodness of fit and graphical comparison of the fit. \cite{leelin} focuses on mixture of Erlang distributions while \cite{asmussenolhsson} provides an algorithm for general phase-type distributions. \cite{leelin} illustrates their algorithm with an uniform, a Weibull, a Pareto and log-normal distributions.

\subsection{Random generation}
From \cite{neuts}, we have the following algorithm to generate phase-type distributed random variate. 
Let $s$ be the state of the underlying Markov chain.
\begin{itemize}
\item $S$ initialized from the discrete distribution characterized by $\pi$
\item $X$ initialized to 0
\item \textbf{while} $S\neq 0$ \textbf{do}
\begin{itemize}
\item generate $U$ from an uniform distribution,
\item $X=X-\frac{1}{\hat \lambda_{ij}}\log(U)$,
\item generate $S$ from a discrete distribution characterized by the row $\hat \Lambda$
\end{itemize}
\end{itemize}
where $\hat \Lambda$ is the transition matrix defined by
$$
\hat \lambda_{ij} = \left\{
\begin{array}{ll}
1 & \txtm{if} i=j=1\\
0 & \txtm{if} i=1 \txtm{and} j\neq 1\\
0 & \txtm{if} i>1 \txtm{and} j=i\\
\frac{\lambda_{ij}}{-\lambda_{ii}} & \txtm{if} i>1 \txtm{and} j\neq i\\
\end{array}
\right. .
$$

\subsection{Applications}
NEED REFERENCE


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Exponential family}
\subsection{Characterization}
\cite{cas} defines the exponential family by the following density or mass probability function
$$
f(x)= e^{d(\theta) e(x) + g(\theta)+h(x)},
$$
where $d, e, g$ and $h$ are known functions and $\theta$ the vector of paremeters. Let us note that the support of the distribution can be $\mathbb R$ or $\mathbb R_+$ or $\mathbb N$. This form for the exponential family is called the natural form.

When we deal with generalized linear models, we use the natural form of the exponential family, which is
$$
f(x) = e^{\frac{\theta x - b(\theta)}{a(\phi)} + c(x,\phi)},
$$
where $a, b, c$ are known functions and $\theta, \phi$\footnote{the canonic and the dispersion parameters.} denote the parameters. This form is derived from the previous by setting $d(\theta)=\theta$, $e(x)=x$ and adding a dispersion parameter $\phi$.

Let $\mu$ be the mean of the variable of an exponential family distribution. We have $\mu=\tau(\theta)$ since $\phi$ is only a dispersion parameter. The mean value form of the exponential family is
$$
f(x) = e^{\frac{\tau^{-1}(\mu) x - b(\tau^{-1}(\mu))}{a(\phi)} + c(x,\phi)}.
$$

\subsection{Properties}
For the exponential family, we have $E(X) = \mu=b'(\theta) $ and $Var(X)=a(\phi) V(\mu) = a(\phi)b''(\theta)$ where $V$ is the unit variance function. The skewness is given by $\gamma_3(X) = \frac{dV}{d\mu}(\mu) \sqrt{\frac{a(\phi)}{V(\mu)}} = \frac{b^{(3)}(\theta)a(\phi)^2}{Var(Y)^{3/2}}$, while the kurtosis is 
$\gamma_4(X) = 3+\left[ \frac{d^2V}{d\mu^2}(\mu)V(\mu)+\left(\frac{dV}{d\mu}(\mu)\right)^2 \right]\frac{a(\phi)}{V(\mu)} = 3+\frac{b^{(4)}(\theta)a(\phi)^3}{Var(Y)^2}$.

The property of uniqueness is the fact that the variance function $V$ uniquely identifies the distribution.

\subsection{Special cases}
The exponential family of distributions in fact contains the most frequently used distributions. Here are the corresponding parameters, listed in a table:

\begin{table}[!htb]
\center
\begin{tabular}{llllll}
\hline
Law & Distribution & $\theta$ & $\phi$ & Expectation & Variance\\
\hline
Normal $\mathcal N(\mu,\sigma^2)$ & $\frac{1}{\sqrt{2\pi \sigma}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}$ & $\mu$ & $\sigma^2$ & $\mu=\theta$ & 1\\
\hline
Gamma $\mathcal G(\alpha,\beta)$ & $\frac{\beta^\alpha x^{\alpha-1}}{\Gamma(\alpha)} e^{-\beta x}$ & $-\frac{\beta}{\alpha} = \frac{1}{\mu}$ & $\frac{1}{\alpha}$ & $\mu=-\frac{1}{\theta}$ & $\mu^2$\\
\hline
Inverse Normal $\mathcal I(\mu,\lambda)$ & $\sqrt{\frac{\lambda}{2\pi x^3}} e^{-\frac{\lambda(x-\mu)^2}{2\mu^2x}}$ & $-\frac{1}{2\mu^2}$ & $\frac{1}{\lambda}$ & $\mu=(-2\theta)^{-\frac{1}{2}}$ & $\mu^3$\\
\hline
Bernoulli $\mathcal B(\mu)$ & $\mu^x(1-\mu)^{1-x}$ & $\log(\frac{\mu}{1-\mu})$ & 1 & $\mu=\frac{e^\theta}{1+e^\theta}$ &  $\mu(1-\mu)$\\
\hline
Poisson $\mathcal P(\mu)$ & $\frac{\mu^x}{x!} e^{-\mu}$ & $\log(\mu)$ & 1 & $\mu=e^\theta$ & $\mu$\\
\hline
Overdispersed Poisson $\mathcal P(\phi,\mu)$ & $\frac{\mu^\frac{x}{\phi}}{\frac{x}{\phi}!} e^{-\mu}$ & $\log(\mu)$ & $\phi$ & $\phi e^\theta$ & $\phi\mu$\\
\hline 
\end{tabular}
\end{table}

\subsection{Estimation}
The log likelihood equations are 
$$
\left\{
\begin{array}{c}
\frac{1}{n} \sum\limits_{i=1}^n \frac{X_i}{a(\phi)} = \frac{b'(\theta)}{a(\phi)}\\
\frac{1}{n} \sum\limits_{i=1}^n \frac{\theta X_ia'(\phi)}{a^2(\phi)} -\frac{1}{n} \sum\limits_{i=1}^n \frac{\partial c}{\partial \phi}(X_i,\phi) = b(\theta)\frac{a'(\phi)}{a^2(\phi)}
\end{array}
\right. ,
$$
for a sample $(X_i)_i$.

\subsection{Random generation}
NEED REFERENCE
\subsection{Applications}
GLM, credibility theory,  lehman scheffe theorem


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Elliptical distribution}
\subsection{Characterization}
TODO

\subsection{Properties}
TODO
\subsection{Special cases}
\subsection{Estimation}
TODO
\subsection{Random generation}
TODO
\subsection{Applications}
